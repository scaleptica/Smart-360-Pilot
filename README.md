# Smart-360-Pilot
There are many limitations to viewing A 360 videos such as 
the requirement of specialized equipment like VR Headsets. 
Another requirement is that the viewer must constantly adjust 
their field of view, either by clicking mouse buttons or by 
adjusting the position of their head. In case the viewer does not 
have the special equipment, the view appears warped and is 
generally a dissatisfactory experience. To tackle these issues, 
the proposed method should take a 360Â° video and using 
computer vision, automatically predict the key area of interest 
in the video. Next, it should use a de-warping algorithm to map 
and flatten the given section into an N-FOV (normal field of 
view) video output. The video generated by this method should 
appear as a normal two-dimensional stream whilst preserving 
as much information as possible.

INSTRUCTIONS TO RUN THE CODE:
1. An input video (360 panoramic mp4) goes in the input directory. A sample video
has already been placed for you.

2. The program runs by running the main.py file

3. The output is produced in the output folder

VARIOUS SPORTING 360 VIDEOS AND THEIR OUTPUTS AFTER RUNNING THROUGH OUR CODE
HAVE ALREADY BEEN GIVEN TO SEE IN THE 'RESULTS OF CODE FOLDER'

Initially the face detection was performed using the "haarcascade_frontalcatface.xml", then in the later commits, the code was updated to use "Multi-task Cascaded Convolutional Networks (MTCNN)" instead. The requirements file consists of the python modules that proved to be the requisites for the project execution

PS: The project was a group effort
